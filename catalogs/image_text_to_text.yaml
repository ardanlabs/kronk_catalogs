catalog: Image-Text-to-Text
models:
  - id: gemma-3-4b-it-q4_0
    category: Image-Text-to-Text
    owned_by: google
    model_family: gemma-3-4b-it-qat-q4_0-gguf
    web_page: https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf
    gated_model: true
    template: gemma-3.jinja
    files:
      models:
        - url: https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/gemma-3-4b-it-q4_0.gguf
          size: 3.16 GiB
      proj:
        url: https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf/resolve/main/mmproj-model-f16-4B.gguf
        size: 851 MiB
    capabilities:
      endpoint: chat_completion
      images: true
      audio: false
      video: true
      streaming: true
      reasoning: false
      tooling: false
      embedding: false
      rerank: false
    metadata:
      created: 2025-04-11T0:00:00Z
      collections: https://huggingface.co/collections/ggml-org
      description: Gemma 3 is a new family of lightweight, open-weight AI models from Google, based on Gemini technology. It supports multiple types of data, such as text, images, and audio. It has a large context window of 128k and supports more than 140 languages.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      flash-attention: enabled

  - id: Qwen2.5-VL-3B-Instruct-Q8_0
    category: Image-Text-to-Text
    owned_by: ggml-org
    model_family: Qwen2.5-VL-3B-Instruct-GGUF
    web_page: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF
    template:
    files:
      models:
        - url: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q8_0.gguf
          size: 3.29 GiB
      proj:
        url: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-Qwen2.5-VL-3B-Instruct-Q8_0.gguf
        size: 845 MiB
    capabilities:
      endpoint: chat_completion
      images: true
      audio: false
      video: true
      streaming: true
      reasoning: false
      tooling: false
      embedding: false
      rerank: false
    metadata:
      created: 2025-04-13T0:00:00Z
      collections: https://huggingface.co/collections/ggml-org
      description: The Qwen2.5-VL-3B-Instruct model is a compact, open-source, instruction-tuned, Vision-Language Model (VLM) balancing performance with efficiency, This model features robust capabilities in interpreting images, charts, and even long videos, enabling object localization, structured data extraction (JSON), and complex visual reasoning.
    config:
      context-window: 8192
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      flash-attention: enabled

  - id: Ministral-3-14B-Instruct-2512-UD-Q8_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Ministral-3-14B-Instruct-2512-GGUF
    web_page: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF
    template: ministral.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-UD-Q8_K_XL.gguf
          size: 15.9 GiB
      proj:
        url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf
        size: 878 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2025-12-02T00:00:00Z
      collections: collections/unsloth
      description:
        Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable
        model in the Mistral AI Ministral 3 family, specifically optimized for edge
        deployment and high-performance local inference.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 512
      cache-type-k: f16
      cache-type-v: f16
      nseq-max: 2
      sampling-parameters:
        temperature: 0.7
        top_p: 0.95

  - id: Ministral-3-14B-Instruct-2512-Q4_0
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Ministral-3-14B-Instruct-2512-GGUF
    web_page: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF
    template: ministral.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-Q4_0.gguf
          size: 7.3 GiB
      proj:
        url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf
        size: 878 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2025-12-02T00:00:00Z
      collections: collections/unsloth
      description:
        Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable
        model in the Mistral AI Ministral 3 family, specifically optimized for edge
        deployment and high-performance local inference.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      sampling-parameters:
        temperature: 0.7
        top_p: 0.95
