catalog: Image-Text-to-Text
models:
  - id: LFM2.5-VL-1.6B-Q8_0
    category: Image-Text-to-Text
    owned_by: unsloth
    architecture: Hybrid
    model_family: LFM2.5-VL-1.6B-GGUF
    web_page: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF
    template: lfm2.5-vl.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-Q8_0.gguf
          size: 1.25 GB
      proj:
        url: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF/resolve/main/mmproj-F16.gguf
        size: 854 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      images: true
    metadata:
      created: 2025-07-11T00:00:00Z
      collections: collections/unsloth
      description:
        LFM2.5-VL-1.6B is Liquid AI's refreshed version of the first vision-language
        model, LFM2-VL-1.6B, built on an updated backbone LFM2.5-1.2B-Base and
        tuned for stronger real-world performance.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 512
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 0.1
        min_p: 0.15

  - id: LFM2.5-VL-1.6B-UD-Q8_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: LFM2.5-VL-1.6B-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF
    template: lfm2.5-vl.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF/resolve/main/LFM2.5-VL-1.6B-UD-Q8_K_XL.gguf
          size: 1.37 GB
      proj:
        url: https://huggingface.co/unsloth/LFM2.5-VL-1.6B-GGUF/resolve/main/mmproj-F16.gguf
        size: 854 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      images: true
    metadata:
      created: 2025-07-11T00:00:00Z
      collections: collections/unsloth
      description:
        LFM2.5-VL-1.6B is Liquid AI's refreshed version of the first vision-language
        model, LFM2-VL-1.6B, built on an updated backbone LFM2.5-1.2B-Base and
        tuned for stronger real-world performance.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 0.1
        min_p: 0.15

  - id: Qwen2.5-VL-3B-Instruct-Q8_0
    category: Image-Text-to-Text
    owned_by: ggml-org
    model_family: Qwen2.5-VL-3B-Instruct-GGUF
    architecture: Dense
    web_page: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF
    template:
    files:
      models:
        - url: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/Qwen2.5-VL-3B-Instruct-Q8_0.gguf
          size: 3.29 GB
      proj:
        url: https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-Qwen2.5-VL-3B-Instruct-Q8_0.gguf
        size: 845 MB
    capabilities:
      endpoint: chat_completion
      images: true
      video: true
      streaming: true
    metadata:
      created: 2025-04-13T0:00:00Z
      collections: https://huggingface.co/ggml-org
      description: The Qwen2.5-VL-3B-Instruct model is a compact, open-source, instruction-tuned, Vision-Language Model (VLM) balancing performance with efficiency, This model features robust capabilities in interpreting images, charts, and even long videos, enabling object localization, structured data extraction (JSON), and complex visual reasoning.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: q8_0
      cache-type-v: q8_0
      flash-attention: enabled

  - id: Ministral-3-14B-Instruct-2512-Q4_0
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Ministral-3-14B-Instruct-2512-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF
    template: ministral.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-Q4_0.gguf
          size: 7.3 GB
      proj:
        url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf
        size: 878 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2025-12-02T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable
        model in the Mistral AI Ministral 3 family, specifically optimized for edge
        deployment and high-performance local inference.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: q8_0
      cache-type-v: q8_0
      sampling-parameters:
        temperature: 0.7
        top_p: 0.95

  - id: Ministral-3-14B-Instruct-2512-UD-Q8_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Ministral-3-14B-Instruct-2512-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF
    template: ministral.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-UD-Q8_K_XL.gguf
          size: 15.9 GB
      proj:
        url: https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/mmproj-F16.gguf
        size: 878 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2025-12-02T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable
        model in the Mistral AI Ministral 3 family, specifically optimized for edge
        deployment and high-performance local inference.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      nseq-max: 2
      sampling-parameters:
        temperature: 0.7
        top_p: 0.95

  - id: Qwen3-VL-30B-A3B-Instruct-Q8_0
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Qwen3-VL-30B-A3B-Instruct-GGUF
    architecture: MoE
    web_page: https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-VL-30B-A3B-Instruct-Q8_0.gguf
          size: 32.5 GB
      proj:
        url: https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF/resolve/main/mmproj-F16.gguf
        size: 1.08 GB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      reasoning: true
    metadata:
      created: 2026-01-01T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        This is a quantized version of Alibaba's Qwen3-VL multimodal large language
        model, specifically designed for efficient inference on resource-constrained
        hardware while maintaining high accuracy.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 0.7
        top_k: 20
        top_p: 0.8

  - id: Qwen3.5-35B-A3B-Q2_K_L
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Qwen3.5-35B-A3B-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF
    template: qwen3.5.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q2_K_L.gguf
          size: 13 GB
      proj:
        url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F16.gguf
        size: 899 MB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      tooling: true
      reasoning: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3.5-35B-A3B-GGUF is a new, highly efficient, and powerful
        medium-sized multimodal model from Alibaba's Qwen team, released in
        February 2026. It is part of the Qwen3.5 family designed for local deployment,
        featuring advanced Mixture-of-Experts (MoE) architecture that delivers high
        performance with fewer activated parameters.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1.0
        top_k: 20
        top_p: 0.95

  - id: Qwen3.5-35B-A3B-UD-Q2_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Qwen3.5-35B-A3B-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF
    template: qwen3.5.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-UD-Q2_K_XL.gguf
          size: 13.6 GB
      proj:
        url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F16.gguf
        size: 899 MB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      tooling: true
      reasoning: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3.5-35B-A3B-GGUF is a new, highly efficient, and powerful
        medium-sized multimodal model from Alibaba's Qwen team, released in
        February 2026. It is part of the Qwen3.5 family designed for local deployment,
        featuring advanced Mixture-of-Experts (MoE) architecture that delivers high
        performance with fewer activated parameters.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1.0
        top_k: 20
        top_p: 0.95

  - id: Qwen3.5-35B-A3B-Q8_0
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Qwen3.5-35B-A3B-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF
    template: qwen3.5.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-Q8_0.gguf
          size: 36.9 GB
      proj:
        url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F16.gguf
        size: 899 MB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      tooling: true
      reasoning: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3.5-35B-A3B-GGUF is a new, highly efficient, and powerful
        medium-sized multimodal model from Alibaba's Qwen team, released in
        February 2026. It is part of the Qwen3.5 family designed for local deployment,
        featuring advanced Mixture-of-Experts (MoE) architecture that delivers high
        performance with fewer activated parameters.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1.0
        top_k: 20
        top_p: 0.95

  - id: Qwen3.5-35B-A3B-UD-Q8_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: Qwen3.5-35B-A3B-UD-Q8_K_XL
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF
    template: qwen3.5.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/Qwen3.5-35B-A3B-UD-Q8_K_XL.gguf
          size: 42.0 GB
      proj:
        url: https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/resolve/main/mmproj-F16.gguf
        size: 899 MB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      tooling: true
      reasoning: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3.5-35B-A3B-GGUF is a new, highly efficient, and powerful
        medium-sized multimodal model from Alibaba's Qwen team, released in
        February 2026. It is part of the Qwen3.5 family designed for local deployment,
        featuring advanced Mixture-of-Experts (MoE) architecture that delivers high
        performance with fewer activated parameters.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1.0
        top_k: 20
        top_p: 0.95

  - id: GLM-4.6V-UD-Q5_K_XL
    category: Image-Text-to-Text
    owned_by: unsloth
    model_family: GLM-4.6V-GGUF
    architecture: MoE
    web_page: https://huggingface.co/unsloth/GLM-4.6V-GGUF
    files:
      models:
        - url: https://huggingface.co/unsloth/GLM-4.6V-GGUF/resolve/main/UD-Q5_K_XL/GLM-4.6V-UD-Q5_K_XL-00001-of-00002.gguf
          size: 49.6 GB
        - url: https://huggingface.co/unsloth/GLM-4.6V-GGUF/resolve/main/UD-Q5_K_XL/GLM-4.6V-UD-Q5_K_XL-00002-of-00002.gguf
          size: 30.7 GB
      proj:
        url: https://huggingface.co/unsloth/GLM-4.6V-GGUF/resolve/main/mmproj-F16.gguf
        size: 30.7 GB
    capabilities:
      endpoint: chat_completion
      images: true
      streaming: true
      reasoning: true
    metadata:
      created: 2025-12-17T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The model GLM-4.6V-UD-Q5_K_XL.gguf is a highly optimized, quantized
        version of the GLM-4.6V multimodal large language model developed by Z.ai.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1
        top_k: 20
        top_p: 0.95
