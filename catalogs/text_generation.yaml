catalog: Text-Generation
models:
  - id: Qwen3-0.6B-Q8_0
    category: Text-Generation
    owned_by: Qwen
    model_family: Qwen3-0.6B-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/Qwen3-0.6B-GGUF
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3-0.6B-GGUF/resolve/918ea7d329d2619ef2e2f3e92943cc561b00ecb5/Qwen3-0.6B-Q8_0.gguf
          size: 639 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      reasoning: true
    metadata:
      created: 2025-05-03T00:00:00Z
      collections: https://huggingface.co/Qwen
      description:
        Qwen3 is the latest generation of large language models in Qwen series,
        offering a comprehensive suite of dense and mixture-of-experts (MoE) models.
    config:
      context-window: 40960
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      nseq-max: 1
      sampling-parameters:
        temperature: 0.7
        top_k: 20
        top_p: 0.8

  - id: LFM2-700M-Q8_0
    category: Text-Generation
    owned_by: unsloth
    model_family: LFM2-700M-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/LFM2-700M-GGUF
    files:
      models:
        - url: https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-Q8_0.gguf
          size: 792 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
    metadata:
      created: 2025-07-11T00:00:00Z
      collections: collections/unsloth
      description:
        LFM2 is a new generation of hybrid models developed by Liquid AI,
        specifically designed for edge AI and on-device deployment. It sets a new standard
        in terms of quality, speed, and memory efficiency.
    config:
      context-window: 32768
      nbatch: 2048
      nubatch: 512
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 0.3
        min_p: 0.15

  - id: LFM2-700M-F16
    category: Text-Generation
    owned_by: unsloth
    architecture: Hybrid
    model_family: LFM2-700M-GGUF
    web_page: https://huggingface.co/unsloth/LFM2-700M-GGUF
    files:
      models:
        - url: https://huggingface.co/unsloth/LFM2-700M-GGUF/resolve/main/LFM2-700M-F16.gguf
          size: 1.5 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
    metadata:
      created: 2025-07-11T00:00:00Z
      collections: collections/unsloth
      description:
        LFM2 is a new generation of hybrid models developed by Liquid AI,
        specifically designed for edge AI and on-device deployment. It sets a new standard
        in terms of quality, speed, and memory efficiency.
    config:
      context-window: 32768
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 0.3
        min_p: 0.15
        enable_thinking: "true"
        reasoning_effort: medium

  - id: rnj-1-instruct-Q6_K
    category: Text-Generation
    owned_by: unsloth
    model_family: rnj-1-instruct-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/rnj-1-instruct-GGUF
    template: rnj-1.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/rnj-1-instruct-GGUF/resolve/main/rnj-1-instruct-Q6_K.gguf
          size: 6.4 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      reasoning: true
      tooling: true
    metadata:
      created: 2025-12-16T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The rnj-1-instruct-Q6_K is a quantized version of the rnj-1-instruct
        8-billion parameter model, specifically optimized for high-performance coding,
        STEM, and agentic workflows, developed by Essential AI.
    config:
      context-window: 32768
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0

  - id: Qwen3-8B-Q8_0
    category: Text-Generation
    owned_by: Qwen
    model_family: Qwen3-8B-GGUF
    architecture: Dense
    web_page: https://huggingface.co/Qwen/Qwen3-8B-GGUF
    files:
      models:
        - url: https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q8_0.gguf
          size: 8.71 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      reasoning: true
      tooling: true
    metadata:
      created: 2025-05-03T00:00:00Z
      collections: https://huggingface.co/Qwen
      description:
        The Qwen3-8B-Q8_0 is a highly efficient, 8.2 billion parameter causal
        language model from the Qwen3 series, heavily quantized using 8-bit
        precision (Q8_0) for improved performance on consumer hardware
    config:
      context-window: 40960
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      nseq-max: 1
      sampling-parameters:
        temperature: 0.7
        top_k: 20
        top_p: 0.8

  - id: gpt-oss-20b-Q8_0
    category: Text-Generation
    owned_by: unsloth
    model_family: gpt-oss-20b-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/gpt-oss-20b-GGUF
    template: gpt-oss.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q8_0.gguf
          size: 12.1 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      reasoning: true
      tooling: true
    metadata:
      created: 2025-08-08T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        OpenAI's GPT-OSS (OpenAI Open-Source Series) are open-weight large
        language models designed for powerful reasoning, agentic tasks, and on-premises
        deployment, released under a permissive Apache 2.0 license.
    config:
      context-window: 98304
      nbatch: 2048
      nubatch: 512
      cache-type-k: q8_0
      cache-type-v: q8_0
      nseq-max: 1
      sampling-parameters:
        temperature: 1
        top_p: 1

  - id: cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0
    category: Text-Generation
    owned_by: bartowski
    model_family: cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF
    architecture: MoE
    web_page: https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF
    template: qwen3-coder.jinja
    files:
      models:
        - url: https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF/resolve/main/cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0.gguf
          size: 26.5 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2025-10-21T00:00:00Z
      collections: https://huggingface.co/bartowski
      description:
        The Qwen3-Coder-30B-A3B-Instruct is a specialized, instruction-tuned
        Mixture-of-Experts (MoE) model for agentic coding tasks. Developed by Alibaba,
        it excels in code generation, understanding large codebases, and using external
        tools with a high degree of efficiency.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      nseq-max: 1
      sampling-parameters:
        temperature: 0.8
        top_k: 20
        top_p: 0.9

  - id: gpt-oss-120b-F16
    category: Text-Generation
    owned_by: unsloth
    model_family: gpt-oss-120b-GGUF
    architecture: Dense
    web_page: https://huggingface.co/unsloth/gpt-oss-120b-GGUF
    template: gpt-oss.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/gpt-oss-120b-GGUF/resolve/main/gpt-oss-120b-F16.gguf
          size: 65.4 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      reasoning: true
      tooling: true
    metadata:
      created: 2025-08-08T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        OpenAI's GPT-OSS (OpenAI Open-Source Series) are open-weight large
        language models designed for powerful reasoning, agentic tasks, and on-premises
        deployment, released under a permissive Apache 2.0 license.
    config:
      context-window: 131072
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      nseq-max: 1
      sampling-parameters:
        temperature: 1
        top_p: 1

  - id: Qwen3-Coder-Next-Q4_0
    category: Text-Generation
    owned_by: unsloth
    model_family: Qwen3-Coder-Next-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF
    template: qwen3-next.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/resolve/main/Qwen3-Coder-Next-Q4_0.gguf
          size: 45.3 MB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3-Coder-Next-GGUF is a highly efficient, 6-bit quantized,
        80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen
        team in February 2026. It is designed specifically for local, privacy-focused
        AI coding agents, offering performance competitive with top-tier, far larger
        models, while running on high-end consumer hardware.
    config:
      context-window: 98304
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1
        top_k: 40
        top_p: 0.95

  - id: Qwen3-Coder-Next-Q8_0
    category: Text-Generation
    owned_by: unsloth
    model_family: Qwen3-Coder-Next-GGUF
    architecture: Hybrid
    web_page: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF
    template: qwen3-next.jinja
    files:
      models:
        - url: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/resolve/main/Q8_0/Qwen3-Coder-Next-Q8_0-00001-of-00003.gguf
          size: 5.94 MB
        - url: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/resolve/main/Q8_0/Qwen3-Coder-Next-Q8_0-00002-of-00003.gguf
          size: 49.8 GB
        - url: https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/resolve/main/Q8_0/Qwen3-Coder-Next-Q8_0-00003-of-00003.gguf
          size: 35.0 GB
    capabilities:
      endpoint: chat_completion
      streaming: true
      tooling: true
    metadata:
      created: 2026-02-03T00:00:00Z
      collections: https://huggingface.co/unsloth
      description:
        The Qwen3-Coder-Next-GGUF is a highly efficient, 6-bit quantized,
        80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen
        team in February 2026. It is designed specifically for local, privacy-focused
        AI coding agents, offering performance competitive with top-tier, far larger
        models, while running on high-end consumer hardware.
    config:
      context-window: 98304
      nbatch: 2048
      nubatch: 2048
      cache-type-k: f16
      cache-type-v: f16
      sampling-parameters:
        temperature: 1
        top_k: 40
        top_p: 0.95
