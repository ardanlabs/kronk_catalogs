# Model Catalog

| ID | Category | Owner | Size | Context | Created | Description | a<br>u<br>d<br>i<br>o | g<br>a<br>t<br>e<br>d | r<br>e<br>a<br>s<br>o<br>n | s<br>t<br>r<br>e<br>a<br>m | t<br>o<br>o<br>l | v<br>i<br>d<br>e<br>o |
|---|---|---|---|---|---|---|:---:|:---:|:---:|:---:|:---:|:---:|
| [bge-reranker-v2-m3-Q8_0](https://huggingface.co/gpustack/bge-reranker-v2-m3-GGUF) | Rerank | ggml-org | 636 MB | 8K | 2024-09-09 | <details><summary>Show</summary>The bge-reranker-v2-m3-GGUF is a quantized, lightweight cross-encoder model developed by the Beijing Academy of Artificial Intelligence (BAAI) for high-performance, multilingual re-ranking tasks. It is designed to take a query and a passage as input and directly output a similarity score (relevance) rather than generating embeddings, allowing for more accurate ranking in RAG (Retrieval-Augmented Generation) systems.</details> |  |  |  |  |  |  |
| [cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0](https://huggingface.co/bartowski/cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF) | Text-Generation | bartowski | 26.5 GB | 128K | 2025-10-21 | <details><summary>Show</summary>The Qwen3-Coder-30B-A3B-Instruct is a specialized, instruction-tuned Mixture-of-Experts (MoE) model for agentic coding tasks. Developed by Alibaba, it excels in code generation, understanding large codebases, and using external tools with a high degree of efficiency.</details> |  |  |  | ✓ | ✓ |  |
| [embeddinggemma-300m-qat-Q8_0](https://huggingface.co/ggml-org/embeddinggemma-300m-qat-q8_0-GGUF) | Embedding | ggml-org | 329 MB | 2K | 2025-09-01 | <details><summary>Show</summary>The embeddinggemma-300M-GGUF model is a quantized version of Google's EmbeddingGemma, a lightweight, open, and multilingual text embedding model designed for efficient performance on on-device hardware like laptops, phones, and desktops.</details> |  |  |  |  |  |  |
| [gemma-3-4b-it-q4_0](https://huggingface.co/google/gemma-3-4b-it-qat-q4_0-gguf) | Image-Text-to-Text | google | 4.0 GB | 128K | 2025-04-11 | <details><summary>Show</summary>Gemma 3 is a new family of lightweight, open-weight AI models from Google, based on Gemini technology. It supports multiple types of data, such as text, images, and audio. It has a large context window of 128k and supports more than 140 languages.</details> |  | ✓ |  | ✓ |  | ✓ |
| [GLM-4.6V-UD-Q2_K_XL](https://huggingface.co/unsloth/GLM-4.6V-GGUF) | Image-Text-to-Text | unsloth | 44.3 GB | 8K | 2025-12-17 | <details><summary>Show</summary>The model GLM-4.6V-UD-Q2_K_XL.gguf is a highly optimized, quantized version of the GLM-4.6V multimodal large language model developed by Z.ai.</details> |  |  | ✓ | ✓ |  |  |
| [GLM-4.6V-UD-Q3_K_XL](https://huggingface.co/unsloth/GLM-4.6V-GGUF) | Image-Text-to-Text | unsloth | 54.5 GB | 8K | 2025-12-17 | <details><summary>Show</summary>The model GLM-4.6V-UD-Q3_K_XL.gguf is a highly optimized, quantized version of the GLM-4.6V multimodal large language model developed by Z.ai.</details> |  |  | ✓ | ✓ |  |  |
| [GLM-4.6V-UD-Q5_K_XL](https://huggingface.co/unsloth/GLM-4.6V-GGUF) | Image-Text-to-Text | unsloth | 111.0 GB | 8K | 2025-12-17 | <details><summary>Show</summary>The model GLM-4.6V-UD-Q5_K_XL.gguf is a highly optimized, quantized version of the GLM-4.6V multimodal large language model developed by Z.ai.</details> |  |  | ✓ | ✓ |  |  |
| [GLM-4.7-Flash-Q8_0](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF) | Text-Generation | unsloth | 31.8 GB | 128K | 2026-01-20 | <details><summary>Show</summary>GLM-4.7-Flash is a high-performance, lightweight Mixture-of-Experts (MoE) model released by Z.ai in January 2026, designed specifically for efficient coding, agentic workflows, and fast local deployment. It is positioned as the strongest in the 30B parameter class, balancing high performance with lower computational requirements.</details> |  |  |  | ✓ | ✓ |  |
| [GLM-4.7-Flash-UD-Q8_K_XL](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF) | Text-Generation | unsloth | 35.1 GB | 128K | 2026-01-20 | <details><summary>Show</summary>GLM-4.7-Flash is a high-performance, lightweight Mixture-of-Experts (MoE) model released by Z.ai in January 2026, designed specifically for efficient coding, agentic workflows, and fast local deployment. It is positioned as the strongest in the 30B parameter class, balancing high performance with lower computational requirements.</details> |  |  |  | ✓ | ✓ |  |
| [gpt-oss-120b-F16](https://huggingface.co/unsloth/gpt-oss-120b-GGUF) | Text-Generation | unsloth | 65.4 GB | 128K | 2025-08-08 | <details><summary>Show</summary>OpenAI's GPT-OSS (OpenAI Open-Source Series) are open-weight large language models designed for powerful reasoning, agentic tasks, and on-premises deployment, released under a permissive Apache 2.0 license.</details> |  |  | ✓ | ✓ | ✓ |  |
| [gpt-oss-20b-Q8_0](https://huggingface.co/unsloth/gpt-oss-20b-GGUF) | Text-Generation | unsloth | 12.1 GB | 96K | 2025-08-08 | <details><summary>Show</summary>OpenAI's GPT-OSS (OpenAI Open-Source Series) are open-weight large language models designed for powerful reasoning, agentic tasks, and on-premises deployment, released under a permissive Apache 2.0 license.</details> |  |  | ✓ | ✓ | ✓ |  |
| [Llama-3.3-70B-Instruct-Q8_0](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF) | Text-Generation | unsloth | 75.0 GB | 128K | 2025-05-10 | <details><summary>Show</summary>Llama 3.3 70B is Meta's advanced, multilingual, open-source large language model (LLM) with 70 billion parameters, excelling in complex reasoning, dialogue, and coding tasks, delivering flagship-level performance (like 405B models) with better efficiency, optimized for text-only applications, and featuring improved instruction-following, safety, and tool-use capabilities for enterprise and research use.</details> |  |  | ✓ | ✓ | ✓ |  |
| [Ministral-3-14B-Instruct-2512-Q4_0](https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF) | Image-Text-to-Text | unsloth | 8.2 GB | 128K | 2025-12-02 | <details><summary>Show</summary>Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable model in the Mistral AI Ministral 3 family, specifically optimized for edge deployment and high-performance local inference.</details> |  |  |  | ✓ | ✓ |  |
| [Ministral-3-14B-Instruct-2512-UD-Q8_K_XL](https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF) | Image-Text-to-Text | unsloth | 16.8 GB | 128K | 2025-12-02 | <details><summary>Show</summary>Ministral-3-14B-Instruct-2512-GGUF is the largest and most capable model in the Mistral AI Ministral 3 family, specifically optimized for edge deployment and high-performance local inference.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen2-Audio-7B.Q8_0](https://huggingface.co/mradermacher/Qwen2-Audio-7B-GGUF) | Audio-Text-to-Text | mradermacher | 8.9 GB | 8K | 2025-06-04 | <details><summary>Show</summary>Qwen2-Audio is the new series of Qwen large audio-language models. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.</details> | ✓ |  |  | ✓ |  |  |
| [Qwen2.5-VL-3B-Instruct-Q8_0](https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF) | Image-Text-to-Text | ggml-org | 4.1 GB | 8K | 2025-04-13 | <details><summary>Show</summary>The Qwen2.5-VL-3B-Instruct model is a compact, open-source, instruction-tuned, Vision-Language Model (VLM) balancing performance with efficiency, This model features robust capabilities in interpreting images, charts, and even long videos, enabling object localization, structured data extraction (JSON), and complex visual reasoning.</details> |  |  |  | ✓ |  | ✓ |
| [Qwen3-8B-Q8_0](https://huggingface.co/Qwen/Qwen3-8B-GGUF) | Text-Generation | Qwen | 8.7 GB | 40K | 2025-05-03 | <details><summary>Show</summary>Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.</details> |  |  | ✓ | ✓ | ✓ |  |
| [Qwen3-Coder-30B-A3B-Instruct-Q8_0](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF) | Text-Generation | unsloth | 32.5 GB | 128K | 2025-08-05 | <details><summary>Show</summary>The Qwen3-Coder-30B-A3B-Instruct is a specialized, instruction-tuned Mixture-of-Experts (MoE) model for agentic coding tasks. Developed by Alibaba, it excels in code generation, understanding large codebases, and using external tools with a high degree of efficiency.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF) | Text-Generation | unsloth | 36.0 GB | 128K | 2025-08-05 | <details><summary>Show</summary>The Qwen3-Coder-30B-A3B-Instruct is a specialized, instruction-tuned Mixture-of-Experts (MoE) model for agentic coding tasks. Developed by Alibaba, it excels in code generation, understanding large codebases, and using external tools with a high degree of efficiency.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-Next-UD-Q2_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Text-Generation | unsloth | 30.2 GB | 96K | 2026-02-03 | <details><summary>Show</summary>The Qwen3-Coder-Next-UD-Q2_K_XL is a highly efficient, 2-bit quantized, 80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen team in February 2026. It is designed specifically for local, privacy-focused AI coding agents, offering performance competitive with top-tier, far larger models, while running on high-end consumer hardware.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-Next-UD-Q4_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Text-Generation | unsloth | 44.6 GB | 96K | 2026-02-03 | <details><summary>Show</summary>The Qwen3-Coder-Next-UD-Q4_K_XL is a highly efficient, 4-bit quantized, 80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen team in February 2026. It is designed specifically for local, privacy-focused AI coding agents, offering performance competitive with top-tier, far larger models, while running on high-end consumer hardware.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-Next-UD-Q5_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Text-Generation | unsloth | 56.8 GB | 96K | 2026-02-03 | <details><summary>Show</summary>The Qwen3-Coder-Next-UD-Q5_K_XL is a highly efficient, 5-bit quantized, 80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen team in February 2026. It is designed specifically for local, privacy-focused AI coding agents, offering performance competitive with top-tier, far larger models, while running on high-end consumer hardware.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-Next-UD-Q6_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Text-Generation | unsloth | 68.6 GB | 96K | 2026-02-03 | <details><summary>Show</summary>The Qwen3-Coder-Next-UD-Q6_K_XL is a highly efficient, 6-bit quantized, 80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen team in February 2026. It is designed specifically for local, privacy-focused AI coding agents, offering performance competitive with top-tier, far larger models, while running on high-end consumer hardware.</details> |  |  |  | ✓ | ✓ |  |
| [Qwen3-Coder-Next-UD-Q8_K_XL](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Text-Generation | unsloth | 93.4 GB | 96K | 2026-02-03 | <details><summary>Show</summary>The Qwen3-Coder-Next-UD-Q8_K_XL is a highly efficient, 8-bit quantized, 80-billion parameter Mixture-of-Experts (MoE) model released by Alibaba's Qwen team in February 2026. It is designed specifically for local, privacy-focused AI coding agents, offering performance competitive with top-tier, far larger models, while running on high-end consumer hardware.</details> |  |  |  | ✓ | ✓ |  |
| [rnj-1-instruct-Q6_K](https://huggingface.co/unsloth/rnj-1-instruct-GGUF) | Text-Generation | unsloth | 6.4 GB | 32K | 2025-12-16 | <details><summary>Show</summary>The rnj-1-instruct-Q6_K is a quantized version of the rnj-1-instruct 8-billion parameter model, specifically optimized for high-performance coding, STEM, and agentic workflows, developed by Essential AI.</details> |  |  | ✓ | ✓ | ✓ |  |
